[
["cph3.html", "Chapter 5 Bivariate OLS: The Foundation of Econometric Analysis 5.1 Estimating a simple regression 5.2 Scatter Plot with Regression Line 5.3 Subsetting Data for Regressions 5.4 Heteroscesdasticity-consistent standard errors. 5.5 Generating Random Numbers 5.6 Simulations", " Chapter 5 Bivariate OLS: The Foundation of Econometric Analysis We will work through the Computing Corner. 5.1 Estimating a simple regression To run a simple regression in R, call the function lm(). 5.1.1 A short detour on some R basics. Some basic principles of R. Everything in R is an object including functions. Everything that happens in R is a function call. function calls act on objects function calls return objects objects returned by function calls may be used in function calls It may help to think of objects as nouns and function calls as verbs. Objects are created with the assignment operator &lt;- (keyboard shortcut: Alt + -). We can name an object almost anything we’d like.1 It is good practice to avoid function names like c, T, mean, etc., to not avoid starting a name with a number, to putting spaces in names. See The tidyverse style guide for an example of a consistent philosophy. We advocate the use of snake case when naming objects with longer names, e.g., we might name GDP per capita as gpd_per_capita. So we might assign 95 to the current temperature like this: current_temp &lt;- 95 Functions act on objects and return objects. Calling a function means to “run” it. All functions have two broad categories of arguments one that supplies the data (object) to be acted on and another to control the details of the computation. Let’s look at the function call mean() as an example. mean(x, trim = 0, na.rm = FALSE, ...) mean has a data argument x and two control arguments trim and na.rm.2 x is an R object, typically a vector. trim is argument that controls the fraction of observations to be ignored on each end of x when calculating the mean. trim has default value of 0 (trim = 0). The na.rm argument controls whether mean ignores NAs in the data. The default value is FALSE (na.rm = FALSE). Since the control arguments have default values, the mean requires you give it only one argument to run, x. Of course, you can change the default values of any control arguments. R evaluates (looks for arguments in) functions in 3 steps. First it uses exact matching of the argument name. If the arguments aren’t named exactly, it uses partial matching of the argument name. Finally, it uses the position of the argument. All arguments do not have to be specified. Arguments not specified will use default values. If an argument necessary to the function call is not used R will throw an error. Good practice in calling functions requires that we specify non-optional arguments first in our function calls and that we specify the names of all optional arguments. Good practice yields easier code auditing and editing and replicability. Let’s look at the ways we can call mean() # let&#39;s assign an object y. y &lt;- c(NA, 1:10, NA) # we can name the arguments exactly mean(x = y, trim = 0, na.rm = TRUE) [1] 5.5 # or we could use the position of the argument mean(y, 0, TRUE) [1] 5.5 # we could use a combination of the two mean(y, na.rm = TRUE) [1] 5.5 Notice in each case mean() returned a vector of length 1, that’s why the [1] precedes the value of the mean. The object returned by a function call can be used as an argument in subsequent function call. For example, suppose we’d like to take the square root of the mean we just calculated. We can pass the object returned by the mean to the function sqrt() as an argument like this: sqrt(mean(y, na.rm = TRUE)) [1] 2.35 We could do this repeatedly. Let’s take the log of that. log(sqrt(mean(y, na.rm = TRUE))) [1] 0.852 As you might imagine, after a while this “nesting” process get’s ugly and difficult to follow fast. This is where the pipe operator %&gt;% (keyboard short-cut Ctrl-Shift-M) shines. The pipe operator %&gt;% works by making the object preceding it the first argument in the function call following it. Let the bar be the firs argument in the function call foo, we could call the function like this foo(bar) or using the pipe operator like this bar %&gt;% foo(). In simple cases like this there isn’t much to gained from the pipe operator.3 The pipe operator shines when multiple function calls are performed in a sequence. It prevents ugly nesting or having to create assign intermediate objects to call functions on later. Below is the code from above using the %&gt;% operator. y %&gt;% mean(na.rm = TRUE) %&gt;% sqrt() %&gt;% log() [1] 0.852 While code is written to get R to do something it is also written for and by human beings. We can translate this code like this “take y, then calculate its mean while removing NA’s, then take the square root, then take the log.” We can find the arguments and their default values, if any, in a variety of ways in R Studio. To estimate a regression, call the function lm(). Let’s find the arguments required by lm() to estimate a simple linear regression. If we simply want a list of the arguments for any function call args() with the function name as the argument.4 args(lm) will return the arguments for the lm function. args(lm) function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...) NULL To get more detailed information on the function call and the arguments use ?function_name. This opens the help page in the Files/Plots/Packages/Help Pane, which can also be accessed by typing “lm” in the search in the same pane. Below we see a portion of the help on lm. ?lm 5.1.2 Estimating an OLS model Estimating a regression using lm requires only one argument, a formula.5 Formulas are structured with a ~. To estimate \\(y_i=\\beta_0+\\beta_1x_i+\\epsilon_i\\) the formula argument takes the form y ~ x. If x and y exist in the global environment, e.g. not in data frame, run the regression by calling lm(y ~ x). # create y and x in the global environment as follows y &lt;- c(275, 141, 70, 75, 310, 80, 160, 263, 205, 185, 170, 155, 145) x &lt;- c(14, 0, 0, 5, 20, 0.75, 0.25, 16, 3, 2, 0.80, 5, 4) # estimate the regression lm(y ~ x) Call: lm(formula = y ~ x) Coefficients: (Intercept) x 121.61 9.22 It is typically impractical to use data (variables) within the global environment. Data sets are typically collected together in data frames. When data “live” in data frame lm requires a second argument to find the data. Let’s estimate the regression \\(\\textit{weight}_i = \\beta_0+\\beta_1\\textit{donuts per week}_i+\\epsilon_i\\). The data are in a data frame named donuts. # including the arguments by name lm(formula = weight ~ donuts_per_week, data = donuts) Call: lm(formula = weight ~ donuts_per_week, data = donuts) Coefficients: (Intercept) donuts_per_week 121.61 9.22 # including the arguments by position lm(weight ~ donuts_per_week, donuts) Call: lm(formula = weight ~ donuts_per_week, data = donuts) Coefficients: (Intercept) donuts_per_week 121.61 9.22 Recall that the pipe operator %&gt;% makes an object the first argument in a function call. So, it appears that we are out of look if we’d like to use it to pipe the data into an lm call. Appearances aside, we can use %&gt;% any function call to insert an object into as an argument in any position in the argument list by using the “pronoun” .. The object will be place wherever the pronoun is. donuts %&gt;% lm(weight ~ donuts_per_week, data = .) Call: lm(formula = weight ~ donuts_per_week, data = .) Coefficients: (Intercept) donuts_per_week 121.61 9.22 # or donuts %&gt;% lm(weight ~ donuts_per_week, .) Call: lm(formula = weight ~ donuts_per_week, data = .) Coefficients: (Intercept) donuts_per_week 121.61 9.22 There are advantages to using %&gt;%. It allows auto-completion of variable names because R knows which data frame we are using. Later we will see that the use of %&gt;% will simplify things like subsetting, creating new variables, etc. 5.1.2.1 Viewing the output lm prints the values of the coefficients. Where are the fitted values, residuals, statistics, etc? Since lm is a function call it must return an object…it does. The object returned by lm is a special kind of vector called a list. This is where the fitted values, ect. “live”. Let’s look at the list. donuts %&gt;% lm(weight ~ donuts_per_week, .) %&gt;% glimpse() List of 12 $ coefficients : Named num [1:2] 121.61 9.22 ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;donuts_per_week&quot; $ residuals : Named num [1:13] 24.26 19.39 -51.61 -92.73 3.92 ... ..- attr(*, &quot;names&quot;)= chr [1:13] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ effects : Named num [1:13] -619.6 215.66 -60.24 -99.06 4.49 ... ..- attr(*, &quot;names&quot;)= chr [1:13] &quot;(Intercept)&quot; &quot;donuts_per_week&quot; &quot;&quot; &quot;&quot; ... $ rank : int 2 $ fitted.values: Named num [1:13] 251 122 122 168 306 ... ..- attr(*, &quot;names&quot;)= chr [1:13] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ assign : int [1:2] 0 1 $ qr :List of 5 ..$ qr : num [1:13, 1:2] -3.606 0.277 0.277 0.277 0.277 ... .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ..$ qraux: num [1:2] 1.28 1.31 ..$ pivot: int [1:2] 1 2 ..$ tol : num 0.0000001 ..$ rank : int 2 ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; $ df.residual : int 11 $ xlevels : Named list() $ call : language lm(formula = weight ~ donuts_per_week, data = .) $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language weight ~ donuts_per_week .. ..- attr(*, &quot;variables&quot;)= language list(weight, donuts_per_week) .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;donuts_per_week&quot; .. ..- attr(*, &quot;order&quot;)= int 1 .. ..- attr(*, &quot;intercept&quot;)= int 1 .. ..- attr(*, &quot;response&quot;)= int 1 .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x000000001dabcc28&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(weight, donuts_per_week) .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;weight&quot; &quot;donuts_per_week&quot; $ model :&#39;data.frame&#39;: 13 obs. of 2 variables: ..$ weight : num [1:13] 275 141 70 75 310 80 160 263 205 185 ... ..$ donuts_per_week: num [1:13] 14 0 0 5 20 0.75 0.25 16 3 2 ... ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language weight ~ donuts_per_week .. .. ..- attr(*, &quot;variables&quot;)= language list(weight, donuts_per_week) .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;donuts_per_week&quot; .. .. ..- attr(*, &quot;order&quot;)= int 1 .. .. ..- attr(*, &quot;intercept&quot;)= int 1 .. .. ..- attr(*, &quot;response&quot;)= int 1 .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x000000001dabcc28&gt; .. .. ..- attr(*, &quot;predvars&quot;)= language list(weight, donuts_per_week) .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;weight&quot; &quot;donuts_per_week&quot; - attr(*, &quot;class&quot;)= chr &quot;lm&quot; The list contains 12 elements. Here are some we are likely to interested in: coefficients residuals fitted values df.residual Let’s assign this list to the object lm_donuts lm_donuts &lt;- donuts %&gt;% lm(weight ~ donuts_per_week, .) We can extract any of the elements of the list with one of three operators $ [ [[ and the name of the element or its position in the list. Each of these operators returns an object $ and [[ return vectors while [ returns a list. Let’s extract the residuals from lm_donuts lm_donuts$residuals 1 2 3 4 5 6 7 8 9 10 11 24.26 19.39 -51.61 -92.73 3.92 -48.53 36.08 -6.19 55.72 44.94 41.01 12 13 -12.73 -13.51 lm_donuts[[2]] 1 2 3 4 5 6 7 8 9 10 11 24.26 19.39 -51.61 -92.73 3.92 -48.53 36.08 -6.19 55.72 44.94 41.01 12 13 -12.73 -13.51 lm_donuts[2] $residuals 1 2 3 4 5 6 7 8 9 10 11 24.26 19.39 -51.61 -92.73 3.92 -48.53 36.08 -6.19 55.72 44.94 41.01 12 13 -12.73 -13.51 5.1.2.2 Regression output summaries There are myriad function calls to “dig out” the elements of an lm list in nicely formatted ways depending on our purpose. Let’s look at a few of them. We can call these functions at the end of a pipeline without the intermediate step of naming the lm object or we can call them on the named object. Let’s do the former for this exercise. 5.1.2.2.1 summary summary from base R extracts the coefficient estimates, their standard errors, their t-statistics, and their p-values. We also get degrees of freedom, r-squared, adjusted r-squared, F-stat for overall significance and some summary stats on the dependent variable. donuts %&gt;% lm(weight ~ donuts_per_week, .) %&gt;% summary() Call: lm(formula = weight ~ donuts_per_week, data = .) Residuals: Min 1Q Median 3Q Max -92.73 -13.51 3.92 36.08 55.72 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 121.61 16.59 7.33 0.000015 *** donuts_per_week 9.22 1.96 4.71 0.00064 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 45.8 on 11 degrees of freedom Multiple R-squared: 0.668, Adjusted R-squared: 0.638 F-statistic: 22.2 on 1 and 11 DF, p-value: 0.000643 5.1.2.2.2 The broom package The broom package has three functions that extract different elements of the lm object and always return a tibble. Rather than loading the broom package with library we will use it direclty by using its name and ::. donuts %&gt;% lm(weight ~ donuts_per_week, .) %&gt;% broom::tidy() # A tibble: 2 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 122. 16.6 7.33 0.0000149 2 donuts_per_week 9.22 1.96 4.71 0.000643 donuts %&gt;% lm(weight ~ donuts_per_week, .) %&gt;% broom::glance() # A tibble: 1 x 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.668 0.638 45.8 22.2 6.43e-4 1 -67.1 140. 142. # ... with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; donuts %&gt;% lm(weight ~ donuts_per_week, .) %&gt;% broom::augment() # A tibble: 13 x 8 weight donuts_per_week .fitted .resid .std.resid .hat .sigma .cooksd &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 275 14 251. 24.3 0.596 0.211 47.3 0.0474 2 141 0 122. 19.4 0.454 0.131 47.6 0.0156 3 70 0 122. -51.6 -1.21 0.131 44.7 0.110 4 75 5 168. -92.7 -2.11 0.0773 37.1 0.186 5 310 20 306. 3.92 0.117 0.464 48.0 0.00591 6 80 0.75 129. -48.5 -1.13 0.117 45.2 0.0844 7 160 0.25 124. 36.1 0.843 0.126 46.5 0.0513 8 263 16 269. -6.19 -0.159 0.281 48.0 0.00495 9 205 3 149. 55.7 1.27 0.0879 44.4 0.0781 10 185 2 140. 44.9 1.03 0.0986 45.7 0.0584 11 170 0.8 129. 41.0 0.952 0.116 46.0 0.0597 12 155 5 168. -12.7 -0.289 0.0773 47.9 0.00350 13 145 4 159. -13.5 -0.308 0.0807 47.8 0.00415 5.1.2.2.3 Publication formatted tables In addition we can also create publication ready tables. Let’s look at the gt package and the stargazer package. donuts %&gt;% lm(weight ~ donuts_per_week, .) %&gt;% broom::tidy() %&gt;% gt::gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #lxlmiqyfjn .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lxlmiqyfjn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lxlmiqyfjn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lxlmiqyfjn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #lxlmiqyfjn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lxlmiqyfjn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lxlmiqyfjn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lxlmiqyfjn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lxlmiqyfjn .gt_column_spanner_outer:first-child { padding-left: 0; } #lxlmiqyfjn .gt_column_spanner_outer:last-child { padding-right: 0; } #lxlmiqyfjn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #lxlmiqyfjn .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #lxlmiqyfjn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lxlmiqyfjn .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lxlmiqyfjn .gt_from_md > :first-child { margin-top: 0; } #lxlmiqyfjn .gt_from_md > :last-child { margin-bottom: 0; } #lxlmiqyfjn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lxlmiqyfjn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #lxlmiqyfjn .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lxlmiqyfjn .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #lxlmiqyfjn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lxlmiqyfjn .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lxlmiqyfjn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lxlmiqyfjn .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lxlmiqyfjn .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #lxlmiqyfjn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lxlmiqyfjn .gt_sourcenote { font-size: 90%; padding: 4px; } #lxlmiqyfjn .gt_left { text-align: left; } #lxlmiqyfjn .gt_center { text-align: center; } #lxlmiqyfjn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lxlmiqyfjn .gt_font_normal { font-weight: normal; } #lxlmiqyfjn .gt_font_bold { font-weight: bold; } #lxlmiqyfjn .gt_font_italic { font-style: italic; } #lxlmiqyfjn .gt_super { font-size: 65%; } #lxlmiqyfjn .gt_footnote_marks { font-style: italic; font-size: 65%; } term estimate std.error statistic p.value (Intercept) 121.61 16.59 7.33 0.0000149 donuts_per_week 9.22 1.96 4.71 0.0006426 donuts %&gt;% lm(weight ~ donuts_per_week, .) %&gt;% stargazer::stargazer(type = &quot;html&quot;) &lt;table style=&quot;text-align:center&quot;&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;em&gt;Dependent variable:&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan=&quot;1&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;weight&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td colspan=&quot;2&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;donuts_per_week&lt;/td&gt;&lt;td&gt;9.220&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;(1.960)&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Constant&lt;/td&gt;&lt;td&gt;122.000&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;(16.600)&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td colspan=&quot;2&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Observations&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.668&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Adjusted R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.638&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;Residual Std. Error&lt;/td&gt;&lt;td&gt;45.800 (df = 11)&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;F Statistic&lt;/td&gt;&lt;td&gt;22.200&lt;sup&gt;***&lt;/sup&gt; (df = 1; 11)&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td colspan=&quot;2&quot; style=&quot;border-bottom: 1px solid black&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;text-align:left&quot;&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/td&gt;&lt;td style=&quot;text-align:right&quot;&gt;&lt;sup&gt;*&lt;/sup&gt;p&lt;0.1; &lt;sup&gt;**&lt;/sup&gt;p&lt;0.05; &lt;sup&gt;***&lt;/sup&gt;p&lt;0.01&lt;/td&gt;&lt;/tr&gt; &lt;/table&gt; There are multiple arguments for each of these function calls to change the appearance of any of the tables. Reading the vignettes for each for guidance. 5.2 Scatter Plot with Regression Line ggplot2 makes adding a fitted regression line to a scatter plot very easy. You need only add a geometry called geom_smooth with the appropriate method argument to plot. The default is to include a confidence interval estimate around the fitted line. To remove the error band include the argument se = FALSE. Let’s start with the scatter diagram. donuts %&gt;% ggplot(aes(x = donuts_per_week, y = weight)) + geom_point() + labs(x = &quot;Donuts per week&quot;, y = &quot;Weight&quot;) Adding the regression line means adding an additional layer. donuts %&gt;% ggplot(aes(x = donuts_per_week, y = weight)) + geom_point() + geom_smooth(method = &quot;lm&quot;) 5.3 Subsetting Data for Regressions There are a few ways to subset data to use in the lm call: the [ operator, the subset argument, or dplyr::filter. 5.3.1 The [ operator The [ operator is the most basic way to subset vectors in R. We can exclude Homer’s observation directly in the lm call like this: lm(weight[name != &quot;Homer&quot;] ~ donuts_per_week[name != &quot;Homer&quot;], data = donuts) %&gt;% summary() Call: lm(formula = weight[name != &quot;Homer&quot;] ~ donuts_per_week[name != &quot;Homer&quot;], data = donuts) Residuals: Min 1Q Median 3Q Max -90.58 -20.99 7.26 37.24 56.90 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 121.87 17.13 7.12 0.000032 *** donuts_per_week[name != &quot;Homer&quot;] 8.74 2.19 4.00 0.0025 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 47.3 on 10 degrees of freedom Multiple R-squared: 0.615, Adjusted R-squared: 0.577 F-statistic: 16 on 1 and 10 DF, p-value: 0.00252 Let’s subset the donuts data frame to exclude Homer’s record. Data frames have two dimensions rows and columns (df[r,c]) To subset a data frame using [ by row, write the requirement for the row in position r and leave position c blank (means use all columns). lm(weight ~ donuts_per_week, data = donuts[donuts$name != &quot;Homer&quot;,]) %&gt;% summary() Call: lm(formula = weight ~ donuts_per_week, data = donuts[donuts$name != &quot;Homer&quot;, ]) Residuals: Min 1Q Median 3Q Max -90.58 -20.99 7.26 37.24 56.90 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 121.87 17.13 7.12 0.000032 *** donuts_per_week 8.74 2.19 4.00 0.0025 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 47.3 on 10 degrees of freedom Multiple R-squared: 0.615, Adjusted R-squared: 0.577 F-statistic: 16 on 1 and 10 DF, p-value: 0.00252 We can pipe this into the lm call. donuts[donuts$name != &quot;Homer&quot;,] %&gt;% lm(weight ~ donuts_per_week, .) Call: lm(formula = weight ~ donuts_per_week, data = .) Coefficients: (Intercept) donuts_per_week 121.87 8.74 5.3.2 Subset argument Subsetting can be directly done with the subset argument in the lm call. To run a regression that excludes the Homer observation, use the option subset = (name != \"Homer\")6 donuts %&gt;% lm(weight ~ donuts_per_week, data = ., subset = (name != &quot;Homer&quot;)) Call: lm(formula = weight ~ donuts_per_week, data = ., subset = (name != &quot;Homer&quot;)) Coefficients: (Intercept) donuts_per_week 121.87 8.74 5.3.3 dplyr::filter Alternatively we can make use of filter from the dplyr package. Recall, filter is the data manipulation verb that chooses observations in a data frame. filter is easier for human reading and auditing. donuts %&gt;% filter(name != &quot;Homer&quot;) %&gt;% lm(weight ~ donuts_per_week, .) Call: lm(formula = weight ~ donuts_per_week, data = .) Coefficients: (Intercept) donuts_per_week 121.87 8.74 To include those observations where weight is greater than 100: donuts %&gt;% filter(weight &gt; 100) %&gt;% lm(weight ~ donuts_per_week, .) Call: lm(formula = weight ~ donuts_per_week, data = .) Coefficients: (Intercept) donuts_per_week 151.05 7.66 5.4 Heteroscesdasticity-consistent standard errors. The estimatr package allows you to directly calculate robust standard errors. Install the package by calling install.packages(\"estimatr\") R Studio allows you to install packages in the Files/Plots/Packages/Help Pane by clicking on the Install icon on the Packages tab; as you type the name of the package, you will see completion suggestions. Choose the package you wish to install and R Studio will install it. Call lm_robust() to estimate an OLS model with robust standard errors with the se_type = \"HC0 argument for the most common method of generating robust standard errors. library(estimatr) donuts %&gt;% lm_robust(weight ~ donuts_per_week, ., se_type = &quot;HC0&quot;) %&gt;% summary() Call: lm_robust(formula = weight ~ donuts_per_week, data = ., se_type = &quot;HC0&quot;) Standard error type: HC0 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF (Intercept) 121.61 15.87 7.66 0.00000983 86.68 156.5 11 donuts_per_week 9.22 1.02 9.07 0.00000194 6.99 11.5 11 Multiple R-squared: 0.668 , Adjusted R-squared: 0.638 F-statistic: 82.3 on 1 and 11 DF, p-value: 0.00000194 donuts %&gt;% lm_robust(weight ~ donuts_per_week, ., se_type = &quot;HC0&quot;) %&gt;% tidy() term estimate std.error statistic p.value conf.low conf.high df 1 (Intercept) 121.61 15.87 7.66 0.00000983 86.68 156.5 11 2 donuts_per_week 9.22 1.02 9.07 0.00000194 6.99 11.5 11 outcome 1 weight 2 weight 5.5 Generating Random Numbers Random numbers can be useful in variety of applications in econometrics. One application is simulation, where we simulate observations to demonstrate properties of OLS estimators, e.g. Once you’ve decided the distribution from which your random numbers will be drawn and the number of draws you wish to make, you will create a vector of those observations. The most intuitive form of random number generation is sample. Suppose you wanted to simulate the role of a single die, use sample(1:6,1) or using the pipe operator 1:6 %&gt;% sample(1). Read the command aloud like this “from the integers 1, 2, 3, 4, 5, 6, choose a sample of size 1.” You can choose larger samples by changing the size argument. The size argument can not be larger than the number of integers unless the default argument replace = FALSE, is changed to replace = TRUE. To generate a simulation of 100 rolls of a single die call 1:6 %&gt;% sample(100, replace = TRUE). Random numbers may be generate from any probability distribution. The random number generator function for a given probability distribution begins with the letter r followed by the name of the distribution in r. To generate uniform random numbers between 0 and 1, use runif, from a normal distribution use rnorm, etc. Use args(distribution name) or ?distribution name to find out more about the necessary arguments for individual distributions. 5.6 Simulations Monte Carlo simulations are a useful tool for understanding how the value of an estimator changes as the sample data changes. Consider the example of rolling a single die n times and calculating the average number of pips on the side-up face of the die. We know that \\(\\bar X\\) is an unbiased estimator of \\(\\mu\\). Recall that any estimator, \\(\\hat\\theta\\) is an unbiased estimator of \\(\\theta\\) if \\(E(\\hat\\theta) = \\theta\\). We can show that \\(E(\\bar X) = \\mu\\). Let \\[\\bar X = \\frac{\\sum{x_i}}{n}\\] Then, \\[\\begin{aligned} E(\\bar{X}) &amp;= E\\left( \\frac{\\sum{x_i}}{n} \\right)\\\\ &amp;= \\frac{1}{n}\\sum{E(x_i)} \\\\ &amp;= \\frac{1}{n}\\sum{\\mu}\\\\ &amp;= \\frac{1}{n}n\\mu\\\\ &amp;= \\mu \\end{aligned}\\] So, we would expect \\(\\bar X = 3.5\\) since \\(\\mu = 3.5\\). Simulating 100 rolls of a single die 1000 times would allow us to look at the sampling distribution of the sample mean. This will allow us to see the range of values that \\(\\bar X\\) might take on. Perform a Monte Carlo simulation by generating many samples, find the value of the estimator, and investigating it’s distribution. We could do this by generating a single sample, calculating the value of the estimator, and repeating the desired number of times. This would be tedious. We can instead make use of the concept of a loop in R. A loop evaluates the same code repeatedly until some threshold is met. There are two types of loops in R, for loops and while loops. A for loop runs the code a specific number of times; a while loop runs the code until a logical condition is met. We will use a for loop to run our simulation. First, instruct R on the number of times to run through the loop. The loop itself is contained between the braces {}. xbar &lt;- numeric() # initialize the vector to store the observations of x bar for(i in 1:1000) { x &lt;- 1:6 %&gt;% sample(100, replace = T) xbar[i] &lt;- mean(x) } xbar %&gt;% mean() # find the mean of the 1000 [1] 3.5 xbar %&gt;% as.data.frame() %&gt;% # coerce xbar to a data frame ggplot(aes(x = xbar)) + # map xbar to x geom_density() + # geom_density creates a &quot;probability distribution&quot; geom_vline(xintercept = 3.5) # place a vertical line at the mean. We could do the same thing with the simple linear regression \\(Y_i = \\beta_0+\\beta_1X_i+\\epsilon_i\\). We know the OLS estimator of \\(\\beta_1\\) is \\(\\hat\\beta_1\\). The value of the estimator, called the estimate, depends upon the particular sample that is drawn. Monte Carlo simulation will allows to see how the estimate changes across many samples. For \\(\\hat\\beta_j\\) to be an unbiased estimator of \\(\\beta_j\\), \\(E(\\hat\\beta_j) = \\beta_j\\). The proof is beyond the scope of this manual, but you will see or have seen the proof. Suppose we perform a Monte Carlo simulation with know values of \\(\\beta_0\\) and \\(\\beta_1\\) where the error term \\(\\epsilon_i\\) is drawn from a normal distribution with a mean of zero and a constant variance, i.e., \\(\\epsilon_i ~ N(0, \\sigma^2)\\), will the estimates be statistically the same as the known parameters. Let’s find out. Suppose the population regression function is \\(y_i = 10 + 3x_i\\), n &lt;- 50 # sample size N &lt;- 1000 # of simulations beta_0 &lt;- 10 # assign the value of the intercept parameter beta_1 &lt;- 3 # assign the value of the slope parameter beta_hat_0 &lt;- numeric(length = N) #initialize the vector for beta0 estimates beta_hat_1 &lt;- numeric(length = N) #initialize the vector for beta1 estimates y &lt;- numeric(length = N) #initialize the vector for the y values # create x randomly x &lt;- 1:10 %&gt;% sample(n, replace = T) # keep it fixed in repeated samples # run the simulation for(i in 1:N) { epsilon &lt;- rnorm(n, 0 , 2) # create the random error y &lt;- beta_0 + beta_1*x + epsilon beta_hat_0[i] &lt;- lm(y ~ x)$coef[1] beta_hat_1[i] &lt;- lm(y ~ x)$coef[2] } # write the betas to a tibble beta_hats &lt;- tibble(beta_hat_0, beta_hat_1) Let’s look at the mean values of the estimators, using dplyr::summarize beta_hats %&gt;% summarize(mean_beta_hat_0 = mean(beta_hat_0), mean_beta_hat_1 = mean(beta_hat_1)) # A tibble: 1 x 2 mean_beta_hat_0 mean_beta_hat_1 &lt;dbl&gt; &lt;dbl&gt; 1 10.0 3.00 Let’s look at the histograms. beta_hats %&gt;% ggplot(aes(x = beta_hat_0)) + geom_histogram(binwidth = .5) + geom_vline(xintercept = 10, color = &quot;red&quot;, size = 1.2) beta_hats %&gt;% ggplot(aes(x = beta_hat_1)) + geom_histogram(bins = 20) + geom_vline(xintercept = 3, color = &quot;red&quot;, size = 1.2) Call ?Reserved to see names that you can’t use.↩︎ The … represent further arguments passed to or from other methods.↩︎ Possibly it is clear to say take bar and do foo to it than to say do foo on bar.↩︎ Remember everything in R is an object–even a function!↩︎ This is one of few functions that does not have the data as its first argument.↩︎ We will use %&gt;% to avoid having to assign the lm object as an intermediate step.↩︎ "]
]
